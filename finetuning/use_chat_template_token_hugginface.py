# -*- coding: utf-8 -*-
"""use_chat_template_token_hugginface.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nS7of1BCB8kgs9dZazIhAJSjZcP5YSql

**Bibliografy:**

*   https://www.determined.ai/blog/llm-finetuning
*    https://github.com/somosnlp/recursos/blob/main/hackathon_2024/entrenamiento_llm_instrucciones.ipynb
"""

!pip install datasets

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

CHAT_ML_TEMPLATE_MEDITRON = """
{% for message in messages %}
{% if message['role'] == 'user' %}
{{'<|im_start|>user\n' + message['content'].strip() + '<|im_end|>' }}
{% elif message['role'] == 'system' %}
{{'<|im_start|>system\n' + message['content'].strip() + '<|im_end|>' }}
{% elif message['role'] == 'assistant' %}
{{'<|im_start|>assistant\n'  + message['content'] + '<|im_end|>' }}
{% endif %}
{% endfor %}
"""
TOPIC_TYPE_FILTER = 'medical_diagnostic'

CHAT_ML_TEMPLATE_BIOMISTRAL = """
{% for message in messages %}
{% if message['role'] == 'user' %}
{{'<|im_start|>user\n' + message['content'].strip() + '<|im_end|>' }}
{% elif message['role'] == 'system' %}
{{'<|im_start|>system\n' + message['content'].strip() + '<|im_end|>' }}
{% elif message['role'] == 'assistant' %}
{{'<|im_start|>assistant\n'  + message['content'] + '<|im_end|>' }}
{% endif %}
{% endfor %}
"""

HF_TOKEN = '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'
from huggingface_hub import login
#Loggin to Huggin Face
login(token = HF_TOKEN)

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v0.4"
token_name = "DeepESP/gpt2-spanish-medium"

tokenizer = AutoTokenizer.from_pretrained(model_name, eos_token="<|im_end|>")
tokenizer.chat_template = CHAT_ML_TEMPLATE_MEDITRON

"""**Defined template assinged**

To make the dataset understandable for our model, we need to do the following for every sample:  


1.   Apply the chat template.
2.   Tokenize the text, and encode the tokens (convert them into integers).

For step 1, the tokenizer comes with a handy function called apply_chat_template. It expects a list of strings and their roles (“system”, “user”, or “assistant”). So first, we need to extract this list from each dataset sample. Let’s write a function that does this for a single sample:
"""

def get_chat_format(element):
    """
    Processes a single sample from the alpaca dataset to structure it for chatbot training.

    This function transforms the dataset sample into a format suitable for training,
    where each message is categorized by its role in the conversation (system, input, user, assistant).
    It initializes the conversation with a system message, then conditionally adds an input message,
    follows with the user's instruction, and finally, the assistant's output based on the provided inputs.

    Parameters
    ----------
    sample : dict
        A dictionary representing a single sample from the dataset. It must contain
        keys corresponding to input and output components of the conversation.

    Returns
    -------
    dict
        A modified dictionary with a 'messages' key that contains a list of ordered messages,
        each annotated with its role in the conversation.
    """

    prompt_template="""A partir del caso clínico que se expone a continuación, tu tarea es la siguiente.
      Como médico experto, tu tarea es la de diagnosticar al paciente en base al caso clínico. Responde únicamente con el diagnóstico para el paciente de forma concisa.
      Caso clínico: {caso_clinico}
      """
      # cómo usarlo con un LLM:

    system_prompt = "Eres un experto en medicina que realiza diagnósticos en base a casos clínicos."

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt_template.format(caso_clinico=element["raw_text"])},
        {"role": "assistant", "content": element["topic"]},
    ]

    element["raw_text"] = messages
    return element

from datasets import load_dataset, concatenate_datasets
spanishMedicaLllmDataset = load_dataset("somosnlp/spanish_medica_llm", split="train")
spanishMedicaLllmDataset

"""***Only select on datase item for topic_type = 'medical_diacnostic'***"""

spanishMedicaLllmDataset = spanishMedicaLllmDataset.filter(lambda example: example["topic_type"] == TOPIC_TYPE_FILTER)
spanishMedicaLllmDataset

"""Remove innecesary columns"""

spanishMedicaLllmDataset = spanishMedicaLllmDataset.remove_columns([col for col in spanishMedicaLllmDataset.features if col not in ['raw_text', 'topic']])

#Appy instruccion chat_template

spanishMedicaLllmDataset = spanishMedicaLllmDataset.map(
    get_chat_format,
    batched=False,
    num_proc=4
    )

spanishMedicaLllmDataset = spanishMedicaLllmDataset.train_test_split(0.2, seed=203984)

formatted = tokenizer.apply_chat_template(
   spanishMedicaLllmDataset['train'][0]['raw_text'],
   tokenize=False
)

formatted