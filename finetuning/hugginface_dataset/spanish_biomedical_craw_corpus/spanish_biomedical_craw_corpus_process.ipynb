{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dionis/SpanishMedicaLLM.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFtxsPeDsZTE",
        "outputId": "1b9547d0-62b4-4ab9-94f2-3ff767bb1728"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SpanishMedicaLLM'...\n",
            "remote: Enumerating objects: 1410, done.\u001b[K\n",
            "remote: Counting objects: 100% (1375/1375), done.\u001b[K\n",
            "remote: Compressing objects: 100% (908/908), done.\u001b[K\n",
            "remote: Total 1410 (delta 502), reused 1259 (delta 417), pack-reused 35\u001b[K\n",
            "Receiving objects: 100% (1410/1410), 48.85 MiB | 9.93 MiB/s, done.\n",
            "Resolving deltas: 100% (506/506), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd SpanishMedicaLLM/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7q87k4lvKN0",
        "outputId": "e2e32e14-87e0-47d9-c77d-c83f4fe61040"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SpanishMedicaLLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd finetuning/hugginface_dataset/spanish_biomedical_craw_corpus/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNleOUfBvOWA",
        "outputId": "f45dbd6b-583e-4c51-d49d-741ec721b738"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SpanishMedicaLLM/finetuning/hugginface_dataset/spanish_biomedical_craw_corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout 3-develop_a_finetunning_test_on_training_process_with_QLora-Epfl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGjsbyxavXSS",
        "outputId": "3c6496d6-2dc7-4176-cefb-78872a3a4d6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch '3-develop_a_finetunning_test_on_training_process_with_QLora-Epfl' set up to track remote branch '3-develop_a_finetunning_test_on_training_process_with_QLora-Epfl' from 'origin'.\n",
            "Switched to a new branch '3-develop_a_finetunning_test_on_training_process_with_QLora-Epfl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "URL = 'https://zenodo.org/records/5513237/files/CoWeSe.txt?download=1'\n",
        "response = request.urlretrieve(URL, \"CoWeSe.txt\")"
      ],
      "metadata": {
        "id": "XRsE_iw1JUQX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WGb685FxA1u",
        "outputId": "5e72e20e-727a-4e49-f77a-022e94c3e968"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yH5nDblMIjoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c650bc1d-bdd6-40c9-a586-7d9fd197207b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "Downloaded all the issues for CoWeSe.txt! Dataset stored at dataset/spanish_medical_llms.jsonl\n",
            " On dataset there are as document  1973048\n",
            " On dataset there are as copy document  0\n",
            " On dataset there are as size of Tokens  1973048\n",
            "File size on Kilobytes (kB) 270594\n",
            "File size on Megabytes  (MB) 264\n",
            "File size on Gigabytes (GB) 0\n",
            "Generating train split: 1973048 examples [00:05, 368953.10 examples/s]\n",
            "Downloading readme: 100% 8.04k/8.04k [00:00<00:00, 21.2MB/s]\n",
            "Downloading data: 100% 23.7M/23.7M [00:00<00:00, 52.0MB/s]\n",
            "Generating train split: 100% 33941/33941 [00:00<00:00, 94956.62 examples/s] \n",
            "Uploading the dataset shards:   0% 0/1 [00:00<?, ?it/s]\n",
            "Creating parquet from Arrow format:   0% 0/2007 [00:00<?, ?ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:   0% 6/2007 [00:00<00:40, 49.88ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:   1% 20/2007 [00:00<00:20, 98.10ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:   2% 50/2007 [00:00<00:10, 185.59ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:   9% 176/2007 [00:00<00:03, 596.42ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  15% 297/2007 [00:00<00:02, 812.60ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  21% 424/2007 [00:00<00:01, 964.03ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  28% 554/2007 [00:00<00:01, 1070.27ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  34% 673/2007 [00:00<00:01, 1105.61ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  40% 793/2007 [00:00<00:01, 1132.45ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  45% 909/2007 [00:01<00:00, 1138.54ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  51% 1024/2007 [00:01<00:00, 1141.13ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  57% 1139/2007 [00:01<00:00, 1115.44ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  63% 1262/2007 [00:01<00:00, 1148.52ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  69% 1378/2007 [00:01<00:00, 1091.05ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  75% 1503/2007 [00:01<00:00, 1134.94ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  81% 1633/2007 [00:01<00:00, 1180.42ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  88% 1759/2007 [00:01<00:00, 1201.19ba/s]\u001b[A\n",
            "Creating parquet from Arrow format:  94% 1880/2007 [00:01<00:00, 1193.59ba/s]\u001b[A\n",
            "Creating parquet from Arrow format: 100% 2007/2007 [00:01<00:00, 1024.03ba/s]\n",
            "Uploading the dataset shards: 100% 1/1 [00:03<00:00,  3.62s/it]\n",
            "README.md: 100% 8.04k/8.04k [00:00<00:00, 26.8MB/s]\n",
            "Dataset({\n",
            "    features: ['raw_text', 'topic', 'speciallity', 'raw_text_type', 'topic_type', 'source', 'country', 'document_id'],\n",
            "    num_rows: 1973048\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "!python using_dataset_hugginface.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tgtTlLB1EfBj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}